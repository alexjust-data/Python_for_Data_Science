{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: right; width: 50%;\">\n",
    "    <p style=\"margin: 0; text-align:right;\">Python en Ciencia de Datos Aplicada</p>\n",
    "    <p style=\"margin: 0; text-align:right; padding-button: 100px;\">Prácticas - Estructuras de datos avanzadas</p>\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"width:100%;\">&nbsp;</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activamos las alertas de estilo\n",
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prácticas - Estructuras de datos\n",
    "\n",
    "## Ejercicio 1\n",
    "\n",
    "Contestad si son Ciertas o Falsas las siguientes preguntas y razonad brevemente la respuesta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(a) Tras ejecutar el siguiente bloque de código, el valor de la variable x es 1 porque las funciones no pueden modificar las variables globales.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 1.1\n",
    "x = 1\n",
    "\n",
    "\n",
    "def square_number(x):\n",
    "    x = x ** 2\n",
    "    return x\n",
    "\n",
    "\n",
    "x = square_number(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respuesta:\n",
    "\n",
    "<span style=\"color: #004ecb\">Falso. En la última línea se realiza una nueva asignación a *x*, por lo que su valor será el que devuelva la función, independientemente de su carácter global.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(b) Una función creada dentro de otra función puede ser utilizada directamente en cualquier parte del código.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respuesta:\n",
    "\n",
    "<span style=\"color: #004ecb\">Falso. Una función declarada dentro de otra estará encapsulada, y solo podrá llamarse dentro de la primera función.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(c) Una función siempre devuelve un objeto, incluso aunque no tenga return.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respuesta:\n",
    "\n",
    "<span style=\"color: #004ecb\">Cierto. Si la función no tiene return devolverá un objeto None.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) No es posible llamar a una función asignando un nombre a los argumentos si hemos declarado que debe recibir argumentos opcionales usando **."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respuesta:\n",
    "\n",
    "<span style=\"color: #004ecb\">Falso. Los argumentos opcionales declarados con ** son argumentos con nombre. Sería cierto si hubiéramos declarado argumentos posicionales, es decir, con *.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2\n",
    "\n",
    "Una agencia periodística ha solicitado nuestra colaboración en una investigación que están llevando a cabo. Para ayudarles, tenemos que crear una función que extraiga direcciones URL de nuestro interés de una colección de tweets.\n",
    "\n",
    "La función recibirá como *input* la ruta que contiene un archivo en formato *.csv* con los tweets y un parámetro opcional. El *output* deberá ser una tupla con el formato `(numero_tweets_url, lista_direcciones)`, donde \n",
    "- `numero_tweets_url` indica el número de tweets que contienen una URL de nuestro interés.\n",
    "- `lista_direcciones` es una lista de los nombres de dominio de las URLs **sin repeticiones**\n",
    "\n",
    "Además, tendremos que ser capaces de ocultar este segundo elemento de la tupla en base al parámetro opcional de la función.\n",
    "\n",
    "A continuación podéis ver algunos ejemplos de formatos de salida de la función. \n",
    "\n",
    "```\n",
    "(2,)\n",
    "(2,['t.co', 'tinyurl.com'])\n",
    "```\n",
    "**Nota:** puede ser interesante consultar información sobre .group() en la documentación de re.\n",
    "\n",
    "En particular, estamos interesados en las URL que poseen el formato *protocolo://dominio/ruta* donde:\n",
    "- Protocolo puede ser http o https.\n",
    "- El dominio está compuesto por una serie de letras con un '.' que separa la extensión (.com, .org, etc).\n",
    "- La ruta es un conjunto de caracteres que puede contener letras o números.\n",
    "\n",
    "Por ejemplo, la web *https://t.co/V3aoj9RUh4* sería una URL de nuestro interés y debería aparecer en el *output* como *'t.co'*. En cambio, la URL *http://www.trump.com/* no nos interesa ya que tiene *www* y carece de *ruta*.\n",
    "\n",
    "\n",
    "El fichero de entrada estará compuesto por un encabezado seguido por un número indeterminado de líneas con el formato\n",
    "```\n",
    "Date,Time,Tweet,Client,Client Simplified\n",
    "```\n",
    "\n",
    "Es decir, cada línea del fichero tras el encabezado contiene un tweet que tenemos que analizar.\n",
    "\n",
    "Encontraréis un ejemplo de fichero de entrada en la carpeta `data/TrumpTweets.csv` extraído de https://data.world/lovesdata/trump-tweets-5-4-09-12-5-16. Se incluye, además, una celda extra con el código que debéis utilizar para comprobar que la función se ejecuta correctamente.\n",
    "\n",
    "**Importante:** Utilizad los principios de **programación funcional** que hemos visto en el Notebook de teoría para resolver este ejercicio y **expresiones regulares** para detectar las URLs de interés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def make_contains_interesting_url(interesting_pattern):\n",
    "    \"\"\"Sets the pattern of interesting URLs\n",
    "\n",
    "    Args:\n",
    "        interesting_pattern: re compiled pattern\n",
    "\n",
    "    Returns:\n",
    "        Function to detect tweets with interesting URLs.\n",
    "    \"\"\"\n",
    "\n",
    "    def contains_interesting_url(tweet):\n",
    "        \"\"\"Determines if a tweet contains an interesting URL.\n",
    "\n",
    "        Args:\n",
    "            tweet: tweet to analyze.\n",
    "\n",
    "        Returns:\n",
    "            A match object if there is an interesting URL. None otherwise.\n",
    "        \"\"\"\n",
    "        return interesting_pattern.search(tweet)\n",
    "\n",
    "    return contains_interesting_url\n",
    "\n",
    "\n",
    "def make_extract_domain(interesting_pattern, all_domains=True):\n",
    "    \"\"\"Sets the pattern of interesting URLs\n",
    "\n",
    "    Args:\n",
    "        interesting_pattern: re compiled pattern\n",
    "        all_domains: whether the domain of all URLs in the tweet\n",
    "                        should be extracted or only the one from the first one.\n",
    "\n",
    "    Returns:\n",
    "        Function to extract domains from tweets.\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_domain(tweet):\n",
    "        \"\"\"Extracts the web domain of the first URL.\n",
    "\n",
    "        Args:\n",
    "            tweet: tweet to analyze.\n",
    "\n",
    "        Returns:\n",
    "            Domain name of interesting URLs.\n",
    "        \"\"\"\n",
    "        if all_domains:\n",
    "            # This option returns the domain of all interesting URLs\n",
    "            return interesting_pattern.findall(tweet)\n",
    "        else:\n",
    "            # This option returns only the domain of the first\n",
    "            # interesting URL in the tweet\n",
    "            return interesting_pattern.search(tweet).group(1)\n",
    "\n",
    "    return extract_domain\n",
    "\n",
    "\n",
    "def find_interesting_urls(file_path, display_domains=False):\n",
    "    \"\"\"Find interesting URLs in tweets.\n",
    "\n",
    "    Args:\n",
    "        file_path: path to the file on disk containing the tweets.\n",
    "        display_domains: indicates whether to display the list\n",
    "                            of interesting domains (default False).\n",
    "\n",
    "    Returns:\n",
    "        Tuple with the number of tweets with interesting URLs and\n",
    "        (optionally) the list of interesting domain names.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise TypeError(\"File does not exist\")\n",
    "\n",
    "    # We use the following regex to check if there is an interesting URL\n",
    "    # https?:// : matches http:// or https://\n",
    "    # [a-zA-Z]+\\.[a-zA-Z]+ : matches the domain name\n",
    "    # /[a-zA-Z]+ : matches the path, note that \\w would include also _\n",
    "    interesting_pattern = re.compile(\n",
    "        r\"https?://([a-zA-Z]+\\.[a-zA-Z]+)/[a-zA-Z0-9]+\"\n",
    "    )\n",
    "\n",
    "    # We add an extra variable to choose between group and findall for\n",
    "    # teaching purposes, but this was not asked in the PEC\n",
    "    all_domains = False\n",
    "\n",
    "    # Add the pattern to the functions using closures\n",
    "    contains_interesting_url = make_contains_interesting_url(\n",
    "        interesting_pattern\n",
    "    )\n",
    "    extract_domain = make_extract_domain(interesting_pattern, all_domains)\n",
    "\n",
    "    # Process the file\n",
    "    tweets = pd.read_csv(file_path)\n",
    "    interesting_tweets = list(\n",
    "        filter(contains_interesting_url, tweets[\"Tweet\"])\n",
    "    )\n",
    "\n",
    "    # If all the domains are extracted we need to flatten the list\n",
    "    if all_domains:\n",
    "        domains = set(\n",
    "            [\n",
    "                item\n",
    "                for sublist in map(extract_domain, interesting_tweets)\n",
    "                for item in sublist\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        domains = set(map(extract_domain, interesting_tweets))\n",
    "\n",
    "    # Define result tuple\n",
    "    if display_domains:\n",
    "        result = len(interesting_tweets), sorted(list(domains))\n",
    "    else:\n",
    "        result = (len(interesting_tweets),)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6939,\n",
       " ['abcn.ws',\n",
       "  'amzn.to',\n",
       "  'aol.it',\n",
       "  'apne.ws',\n",
       "  'bit.ly',\n",
       "  'bloom.bg',\n",
       "  'cnb.cx',\n",
       "  'cs.pn',\n",
       "  'goo.gl',\n",
       "  'nyti.ms',\n",
       "  'ow.ly',\n",
       "  't.co',\n",
       "  'tinyurl.com',\n",
       "  'twitter.com',\n",
       "  'wapo.st'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "find_interesting_urls(\"data/TrumpTweets.csv\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3\n",
    "\n",
    "Nuestro equipo va a analizar cómo se propagó la COVID-19 por el mundo durante la primera ola. Para ello, disponemos de un archivo en la carpeta `data` llamado `time_series_covid19_confirmed_global.csv` extraído de https://github.com/CSSEGISandData/COVID-19.\n",
    "\n",
    "Debemos crear una función que reciba como *input* el path a ese archivo, es decir, `data/time_series_covid19_confirmed_global.csv` y una fecha en formato string (por ejemplo, \"01-06-20\"). La función debe producir un **diccionario** guardado en un **pickle** con el número de casos diarios por país para datos anteriores al 1 de junio del 2020 (sin incluir).\n",
    "\n",
    "En un análisis preliminar del archivo hemos visto que algunos países muestran sus datos por *Province/State* mientras que otros por *Country/Region*. Así, lo primero que tendremos que hacer es agrupar por *Country/Region* y sumar el número de casos diario en cada país. Después, nos quedaremos solo con los datos anteriores al 1 de junio del 2020 (sin incluir). Finalmente, crearemos un diccionario con la estructura:\n",
    "\n",
    "```\n",
    "{\n",
    "\"Pais1\": {\"time\": [1/22/20, 1/23/20,...], \"cases\": [0, 0,...]},\n",
    "\"Pais2\": {\"time\": [1/22/20, 1/23/20,...], \"cases\": [0, 0,...]},\n",
    "...\n",
    "}\n",
    "```\n",
    "**Nota:** hay que tener en cuenta que las fechas están en formato americano, es decir, m/d/y.\n",
    "\n",
    "Una vez terminado el diccionario, lo guardaremos en un pickle llamado `primera_ola.pkl`.  Se incluye, además, una celda extra con el código que debéis utilizar para comprobar que la función se ejecuta correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def build_dictionary(data, max_date):\n",
    "    \"\"\"Create a dictionary with the number of cases per country.\n",
    "\n",
    "    Args:\n",
    "        data: input data.\n",
    "        max_date: maximum date to be stored.\n",
    "\n",
    "    Return:\n",
    "        Dictionary with the number of cases per country.\n",
    "    \"\"\"\n",
    "    # Transform date to datetime format\n",
    "    max_date = pd.to_datetime(max_date, format=\"%d-%m-%y\").date()\n",
    "    # Remove Lat and Long columns\n",
    "    data = data.drop([\"Lat\", \"Long\"], axis=1)\n",
    "    # Group data per country\n",
    "    data = data.groupby([\"Country/Region\"]).sum()\n",
    "    # Filter columns by date\n",
    "    data = data.loc[\n",
    "        :,\n",
    "        [\n",
    "            date < max_date\n",
    "            for date in pd.to_datetime(data.columns, format=\"%m/%d/%y\")\n",
    "        ],\n",
    "    ]\n",
    "    # Iterate over rows, extracting the country and the number of cases\n",
    "    country_cases = dict()\n",
    "    for country, cases in data.iterrows():\n",
    "        country_cases[country] = {\n",
    "            \"time\": list(data.columns.values),\n",
    "            \"cases\": list(cases),\n",
    "        }\n",
    "\n",
    "    return country_cases\n",
    "\n",
    "\n",
    "def create_covid_pickle(input_file, max_date):\n",
    "    \"\"\"Creates a pickle with the number of COVID-19 cases per country.\n",
    "\n",
    "    Args:\n",
    "        input_file: path of the file containing the data\n",
    "        max_date: maximum date to be stored\n",
    "    \"\"\"\n",
    "    # Check if the file exists\n",
    "    if not os.path.isfile(input_file):\n",
    "        raise FileNotFoundError(\"input file not found\")\n",
    "\n",
    "    # Read data\n",
    "    data = pd.read_csv(input_file)\n",
    "\n",
    "    # Create dictionary\n",
    "    country_cases = build_dictionary(data, max_date)\n",
    "\n",
    "    # Export to pickle\n",
    "    with open(\"primera_ola.pkl\", \"wb\") as fout:\n",
    "        pickle.dump(country_cases, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of countries is 192\n",
      "The timeseries of Andorra is {'time': ['1/22/20', '1/23/20', '1/24/20', '1/25/20', '1/26/20', '1/27/20', '1/28/20', '1/29/20', '1/30/20', '1/31/20', '2/1/20', '2/2/20', '2/3/20', '2/4/20', '2/5/20', '2/6/20', '2/7/20', '2/8/20', '2/9/20', '2/10/20', '2/11/20', '2/12/20', '2/13/20', '2/14/20', '2/15/20', '2/16/20', '2/17/20', '2/18/20', '2/19/20', '2/20/20', '2/21/20', '2/22/20', '2/23/20', '2/24/20', '2/25/20', '2/26/20', '2/27/20', '2/28/20', '2/29/20', '3/1/20', '3/2/20', '3/3/20', '3/4/20', '3/5/20', '3/6/20', '3/7/20', '3/8/20', '3/9/20', '3/10/20', '3/11/20', '3/12/20', '3/13/20', '3/14/20', '3/15/20', '3/16/20', '3/17/20', '3/18/20', '3/19/20', '3/20/20', '3/21/20', '3/22/20', '3/23/20', '3/24/20', '3/25/20', '3/26/20', '3/27/20', '3/28/20', '3/29/20', '3/30/20', '3/31/20', '4/1/20', '4/2/20', '4/3/20', '4/4/20', '4/5/20', '4/6/20', '4/7/20', '4/8/20', '4/9/20', '4/10/20', '4/11/20', '4/12/20', '4/13/20', '4/14/20', '4/15/20', '4/16/20', '4/17/20', '4/18/20', '4/19/20', '4/20/20', '4/21/20', '4/22/20', '4/23/20', '4/24/20', '4/25/20', '4/26/20', '4/27/20', '4/28/20', '4/29/20', '4/30/20', '5/1/20', '5/2/20', '5/3/20', '5/4/20', '5/5/20', '5/6/20', '5/7/20', '5/8/20', '5/9/20', '5/10/20', '5/11/20', '5/12/20', '5/13/20', '5/14/20', '5/15/20', '5/16/20', '5/17/20', '5/18/20', '5/19/20', '5/20/20', '5/21/20', '5/22/20', '5/23/20', '5/24/20', '5/25/20', '5/26/20', '5/27/20', '5/28/20', '5/29/20', '5/30/20', '5/31/20'], 'cases': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 39, 39, 53, 75, 88, 113, 133, 164, 188, 224, 267, 308, 334, 370, 376, 390, 428, 439, 466, 501, 525, 545, 564, 583, 601, 601, 638, 646, 659, 673, 673, 696, 704, 713, 717, 717, 723, 723, 731, 738, 738, 743, 743, 743, 745, 745, 747, 748, 750, 751, 751, 752, 752, 754, 755, 755, 758, 760, 761, 761, 761, 761, 761, 761, 762, 762, 762, 762, 762, 763, 763, 763, 763, 764, 764, 764]}\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "create_covid_pickle(\n",
    "    \"data/time_series_covid19_confirmed_global.csv\", \"01-06-20\"\n",
    ")\n",
    "# Load and print some data\n",
    "country_cases = pickle.load(open(\"primera_ola.pkl\", \"rb\"))\n",
    "print(\"The number of countries is {}\".format(len(country_cases)))\n",
    "print(\"The timeseries of Andorra is {}\".format(country_cases[\"Andorra\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 4\n",
    "\n",
    "Una importante compañía del sector de la alimentación nos ha pedido que hagamos un análisis sobre la evolución de los patrones alimenticios de la población. Como no nos han dejado claro en qué intervalo están interesados, hemos decidido empezar por el principio.\n",
    "\n",
    "Hemos conseguido una copia del libro *Arte de cozina, pasteleria, vizcocheria, y conserueria* escrito en 1611 por Francisco Martínez Montiño [aquí](https://archive.org/details/artedecocinapast00mart_0/page/330/mode/2up). Afortunadamente, una compañera se ha encargado ya de digitalizar el libro, extraer todas sus recetas, guardarlas en archivos **txt** y meterlas en un **zip**. Ahora nos toca a nosotros organizarlas un poco. Como tendremos que repetir la operación con más libros, vamos a crear una función.\n",
    "\n",
    "Queremos crear una función que reciba como *input* el path del archivo zip, que será de la forma `data/nombreDelLibro.zip`. La función tendrá que descomprimir el fichero y organizar las recetas en directorios en función del tipo de plato. Además, las carnes tendremos que organizarlas en función del animal, de forma que la estructura de directorios será:\n",
    "\n",
    "```\n",
    "<nombre_del_libro>\n",
    "\n",
    "    <categoria_1>\n",
    "\n",
    "        receta_categoria_1_1.txt\n",
    "        receta_categoria_1_2.txt\n",
    "        ...\n",
    "\n",
    "    <categoria_2>\n",
    "        receta_categoria_2_1.txt\n",
    "        receta_categoria_2_2.txt    \n",
    "        ...\n",
    "        \n",
    "    <carnes>\n",
    "       \n",
    "        <tipo_de_carne_1>\n",
    "            receta_de_carne_1_1.txt\n",
    "            receta_de_carne_1_2.txt\n",
    "            ...\n",
    "        ...\n",
    "    ....\n",
    "```\n",
    "**Nota:** no hay que cambiar el nombre de los archivos de las recetas, solo clasificarlos en el directorio adecuado.\n",
    "\n",
    "Nuestra compañera dice que nos ha dejado dentro del zip un archivo en formato **csv** que contiene el tipo de plato al que se corresponde cada receta. Este archivo usa como separador el símbolo **;** y correctamente leído tiene la siguiente estructura:\n",
    "\n",
    "```\n",
    "title                  type\n",
    "anades_estofadas       carnes-anade\n",
    "pastel_de_caracoles    empanadas_pasteles_y_masas\n",
    "vizcocho_sin_harina    dulces\n",
    "...\n",
    "```\n",
    "Como vemos, el tipo de carne está después del símbolo **-**. Nos han asegurado que es la única categoría con ese símbolo.\n",
    "\n",
    "Finalmente, tenemos que guardar la carpeta que contiene el libro correctamente clasificado en un nuevo archivo **zip** que se llamará `nombreDelLibro_ordenado.zip` y borrar cualquier directorio temporal que hayamos creado. Así, será fácil enviárselo a nuestros compañeros de text mining para que arreglen los problemas del OCR ([¿qué es el OCR?](https://towardsdatascience.com/what-is-ocr-7d46dc419eb9)) y traten de actualizar la ortografía y el vocabulario a algo más moderno.\n",
    "\n",
    "Se incluye, además, una celda extra con el código que debéis utilizar para comprobar que la función se ejecuta correctamente.\n",
    " \n",
    "**Nota:** para crear un zip de todo un directorio se recomienda mirar la documentación de la función make_archive de la librería shutil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import zipfile as zf\n",
    "\n",
    "\n",
    "def order_book(input_file):\n",
    "    \"\"\"Extracts recipes from zip file, classifies them and creates a new zip.\n",
    "\n",
    "    Args:\n",
    "        input_file: path to the zip file\n",
    "    \"\"\"\n",
    "    # Obtain the name of the book\n",
    "    book_name = input_file.split(\"/\")[-1].replace(\".zip\", \"\")\n",
    "\n",
    "    # Extract zip file in a new folder\n",
    "    extract_zip(input_file, book_name)\n",
    "\n",
    "    # Read file with recipes and categories\n",
    "    recipes = read_csv_file(book_name)\n",
    "\n",
    "    # Classify recipes\n",
    "    recipes.apply(classify_recipe, axis=1, book_name=book_name)\n",
    "\n",
    "    # Compress the new folder\n",
    "    compress_file(book_name)\n",
    "\n",
    "    # Delete the book folder\n",
    "    shutil.rmtree(book_name)\n",
    "\n",
    "\n",
    "def extract_zip(zip_file, book_name):\n",
    "    \"\"\"Extracts all the recipes from the zip file.\n",
    "\n",
    "    Args:\n",
    "        zip_file: path of the zip file.\n",
    "        book_name: name of the book being processed.\n",
    "    \"\"\"\n",
    "    # Check the existence of the zip file\n",
    "    if not os.path.isfile(zip_file):\n",
    "        raise FileNotFoundError(\"zip file does not exist\")\n",
    "\n",
    "    # Delete book folder if it already exists\n",
    "    if os.path.isdir(book_name):\n",
    "        shutil.rmtree(book_name)\n",
    "\n",
    "    # Create new directory and extract all files\n",
    "    with zf.ZipFile(zip_file, \"r\") as zip_f:\n",
    "        zip_f.extractall(book_name)\n",
    "\n",
    "\n",
    "def read_csv_file(book_name):\n",
    "    \"\"\"Finds the csv file and reads it.\n",
    "\n",
    "    Args:\n",
    "        book_name -- path of the folder with the book files.\n",
    "\n",
    "    Returns:\n",
    "        Pandas dataframe with the recipes and their categories.\n",
    "    \"\"\"\n",
    "    # Locate the csv file\n",
    "    csv_file = glob.glob(os.path.join(book_name, \"*.csv\"))\n",
    "\n",
    "    # Check that we found it\n",
    "    if not csv_file:\n",
    "        raise FileNotFoundError(\"csv file not found\")\n",
    "\n",
    "    # Read csv file\n",
    "    data = pd.read_csv(csv_file[0], sep=\";\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def classify_recipe(row, book_name):\n",
    "    \"\"\"Move each recipe to the appropriate folder, creating it\n",
    "    if it does not exist.\n",
    "\n",
    "    Args:\n",
    "        row: row containing recipe name and type.\n",
    "        book_name: name of the book being processed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build recipe path\n",
    "    recipe_path = os.path.join(book_name, \"{}.txt\".format(row[\"name\"]))\n",
    "\n",
    "    # Check if recipe exists\n",
    "    if not os.path.isfile(recipe_path):\n",
    "        raise FileNotFoundError(\"recipe does not exist\")\n",
    "\n",
    "    # Change type name to path if there is a \"-\"\n",
    "    type_path = os.path.join(book_name, row[\"type\"].replace(\"-\", \"/\"))\n",
    "\n",
    "    # Create category folder if it does not exist\n",
    "    if not os.path.isdir(type_path):\n",
    "        os.makedirs(type_path)\n",
    "\n",
    "    # Move recipe to its folder\n",
    "    new_recipe_path = os.path.join(type_path, \"{}.txt\".format(row[\"name\"]))\n",
    "    os.rename(recipe_path, new_recipe_path)\n",
    "\n",
    "\n",
    "def compress_file(book_name):\n",
    "    \"\"\"Creates a new zip file with the recipes classified.\n",
    "\n",
    "    Args:\n",
    "        book_name: name of the folder containing the book\n",
    "    \"\"\"\n",
    "    # Check if the folder exists\n",
    "    if not os.path.isdir(book_name):\n",
    "        raise FileNotFoundError(\"book folder does not exist\")\n",
    "\n",
    "    # Create new zip file\n",
    "    shutil.make_archive(\"{}_ordered\".format(book_name), \"zip\", book_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arroces                         3.13 KB\n",
       "carnes                        100.19 KB\n",
       "conservas                      12.54 KB\n",
       "dulces                         58.25 KB\n",
       "empanadas_pasteles_y_masas    114.99 KB\n",
       "enfermos_y_pobres              12.28 KB\n",
       "fiambres                        6.07 KB\n",
       "frutas                         17.08 KB\n",
       "huevos                         28.60 KB\n",
       "jaleas                          6.85 KB\n",
       "lacteos                         9.52 KB\n",
       "migas                           4.55 KB\n",
       "pescados                       19.65 KB\n",
       "salsas                          1.32 KB\n",
       "sopas_potajes_y_cazuelas       39.67 KB\n",
       "tecnicas                       57.12 KB\n",
       "varios                          8.60 KB\n",
       "verduras                       34.93 KB\n",
       "                               20.64 KB\n",
       "carnes/anade                    2.67 KB\n",
       "carnes/ave                      7.20 KB\n",
       "carnes/cabrito                 16.07 KB\n",
       "carnes/capon                    5.62 KB\n",
       "carnes/caracol                  1.27 KB\n",
       "carnes/carnero                 12.45 KB\n",
       "carnes/cerdo                    4.41 KB\n",
       "carnes/conejo                   4.44 KB\n",
       "carnes/embutidos                1.54 KB\n",
       "carnes/gallina                  3.07 KB\n",
       "carnes/grulla                   1.73 KB\n",
       "carnes/lechon                   0.16 KB\n",
       "carnes/liebre                   4.11 KB\n",
       "carnes/paloma                   5.24 KB\n",
       "carnes/perdiz                   1.95 KB\n",
       "carnes/pichon                   3.94 KB\n",
       "carnes/pollo                    8.55 KB\n",
       "carnes/ternera                 13.41 KB\n",
       "carnes/varios                   1.43 KB\n",
       "carnes/venado                   0.92 KB\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Test\n",
    "order_book(\"data/arteDeCozina.zip\")\n",
    "\n",
    "# Read file\n",
    "zp = zf.ZipFile(\"arteDeCozina_ordered.zip\")\n",
    "\n",
    "# Size of each folder\n",
    "sizes = defaultdict(lambda: 0)\n",
    "for folder, size in [\n",
    "    (os.path.dirname(zinfo.filename), zinfo.file_size)\n",
    "    for zinfo in zp.infolist()\n",
    "]:\n",
    "    # Transform Bytes to KBytes\n",
    "    sizes[folder] += size / 1024\n",
    "    if \"carnes\" in folder:\n",
    "        sizes[\"carnes\"] += size / 1024\n",
    "\n",
    "pd.Series(sizes).apply(lambda s: f\"{s:.2f} KB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
